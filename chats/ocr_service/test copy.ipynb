{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from models import OCRModels\n",
    "from document_process import DocumentProcessor\n",
    "\n",
    "# ======================\n",
    "# 1. OCR\n",
    "# ======================\n",
    "\n",
    "models = OCRModels(device=\"gpu\")\n",
    "doc_processor = DocumentProcessor(models)\n",
    "\n",
    "img_path = \"/media/tom/Code/pcb_defect/ProdVision_Server/chats/outputs/image copy.png\"\n",
    "document_paragraphs = doc_processor.process_document(img_path)\n",
    "\n",
    "document_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a2451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_text_patch_cv2(\n",
    "    patch_position,\n",
    "    text_dict,\n",
    "    font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale=0.7,\n",
    "    thickness=1,\n",
    "    text_color=(0, 0, 0),\n",
    "    padding=(10, 30),\n",
    "    line_spacing=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Vẽ text lên nền trắng theo patch_position bằng OpenCV\n",
    "\n",
    "    Args:\n",
    "        patch_position (list): [x_min, y_min, x_max, y_max]\n",
    "        text_dict (list): list các dict chứa key 'text'\n",
    "        font: cv2 font\n",
    "        font_scale (float): scale font\n",
    "        thickness (int): độ dày chữ\n",
    "        text_color (tuple): màu chữ (B, G, R)\n",
    "        padding (tuple): (x_padding, y_start)\n",
    "        line_spacing (int): khoảng cách giữa các dòng\n",
    "\n",
    "    Returns:\n",
    "        img (np.ndarray): ảnh nền trắng đã vẽ text\n",
    "    \"\"\"\n",
    "\n",
    "    x_min, y_min, x_max, y_max = patch_position\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "\n",
    "    # nền trắng\n",
    "    img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    x_pad, y = padding\n",
    "\n",
    "    for item in text_dict:\n",
    "        text = item.get(\"text\", \"\")\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            text,\n",
    "            (x_pad, y),\n",
    "            font,\n",
    "            font_scale,\n",
    "            text_color,\n",
    "            thickness,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        y += line_spacing\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea54498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_table_patch_cv2(\n",
    "    patch_position,\n",
    "    text_dict,\n",
    "    font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale=0.6,\n",
    "    thickness=1,\n",
    "    text_color=(0, 0, 0),\n",
    "    line_color=(0, 0, 0),\n",
    "    cell_padding=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Vẽ table lên nền trắng theo patch_position bằng OpenCV\n",
    "\n",
    "    Args:\n",
    "        patch_position (list): [x_min, y_min, x_max, y_max]\n",
    "        text_dict (list): list các row, mỗi row là dict {col_idx: text}\n",
    "        font: cv2 font\n",
    "        font_scale (float)\n",
    "        thickness (int)\n",
    "        text_color (tuple): màu chữ (BGR)\n",
    "        line_color (tuple): màu đường kẻ bảng\n",
    "        cell_padding (int)\n",
    "\n",
    "    Returns:\n",
    "        img (np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    x_min, y_min, x_max, y_max = patch_position\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "\n",
    "    # nền trắng\n",
    "    img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # số hàng, số cột\n",
    "    rows = len(text_dict)\n",
    "    cols = max(max(row.keys()) for row in text_dict) + 1\n",
    "\n",
    "    row_h = height // rows\n",
    "    col_w = width // cols\n",
    "\n",
    "    # vẽ grid\n",
    "    for r in range(rows + 1):\n",
    "        y = r * row_h\n",
    "        cv2.line(img, (0, y), (width, y), line_color, 1)\n",
    "\n",
    "    for c in range(cols + 1):\n",
    "        x = c * col_w\n",
    "        cv2.line(img, (x, 0), (x, height), line_color, 1)\n",
    "\n",
    "    # vẽ text từng ô\n",
    "    for r, row in enumerate(text_dict):\n",
    "        for c in range(cols):\n",
    "            text = row.get(c, \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            x_text = c * col_w + cell_padding\n",
    "            y_text = r * row_h + row_h // 2\n",
    "\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                text,\n",
    "                (x_text, y_text),\n",
    "                font,\n",
    "                font_scale,\n",
    "                text_color,\n",
    "                thickness,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c858bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = draw_table_patch_cv2(\n",
    "    patch_position=document_paragraphs[0][\"patch_position\"],\n",
    "    text_dict=document_paragraphs[0][\"text_dict\"]\n",
    ")\n",
    "\n",
    "cv2.imwrite(\"table_patch.png\", img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dd5b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/miniconda3/envs/tomkey/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n",
      "/home/tom/miniconda3/envs/tomkey/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/PP-LCNet_x1_0_doc_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-DocLayout-L', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/PP-DocLayout-L`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_table_cls', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/PP-LCNet_x1_0_table_cls`.\u001b[0m\n",
      "\u001b[32mCreating model: ('SLANeXt_wired', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/SLANeXt_wired`.\u001b[0m\n",
      "\u001b[32mCreating model: ('SLANeXt_wireless', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/SLANeXt_wireless`.\u001b[0m\n",
      "\u001b[32mCreating model: ('RT-DETR-L_wired_table_cell_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/RT-DETR-L_wired_table_cell_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('RT-DETR-L_wireless_table_cell_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/RT-DETR-L_wireless_table_cell_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv4_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/PP-OCRv4_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv4_server_rec_doc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/tom/.paddlex/official_models/PP-OCRv4_server_rec_doc`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import TableRecognitionPipelineV2\n",
    "\n",
    "pipeline = TableRecognitionPipelineV2()\n",
    "# ocr = TableRecognitionPipelineV2(use_doc_orientation_classify=True) # Specify whether to use the document orientation classification model with use_doc_orientation_classify\n",
    "# ocr = TableRecognitionPipelineV2(use_doc_unwarping=True) # Specify whether to use the text image unwarping module with use_doc_unwarping\n",
    "# ocr = TableRecognitionPipelineV2(device=\"gpu\") # Specify the device to use GPU for model inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd64049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m{'res': {'input_path': '/media/tom/Code/pcb_defect/ProdVision_Server/media/data_test/1507.05717v1_page-0008.jpg', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_layout_detection': True, 'use_ocr_model': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 0}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9867656230926514, 'coordinate': [np.float32(1329.9775), np.float32(242.50218), np.float32(2400.7954), np.float32(1077.8878)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9853912591934204, 'coordinate': [np.float32(1318.7043), np.float32(1632.1577), np.float32(2410.1753), np.float32(2314.5957)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9853912591934204, 'coordinate': [np.float32(179.79784), np.float32(1752.3248), np.float32(1229.8732), np.float32(2483.551)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9795007705688477, 'coordinate': [np.float32(1319.4156), np.float32(2329.913), np.float32(2404.3787), np.float32(2744.479)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9782090187072754, 'coordinate': [np.float32(171.72852), np.float32(230.63844), np.float32(1247.1311), np.float32(598.1243)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9773460626602173, 'coordinate': [np.float32(1317.1416), np.float32(2760.744), np.float32(2391.0793), np.float32(3075.5432)]}, {'cls_id': 2, 'label': 'text', 'score': 0.9768075942993164, 'coordinate': [np.float32(1324.7856), np.float32(1093.2236), np.float32(2405.3303), np.float32(1457.0403)]}, {'cls_id': 1, 'label': 'image', 'score': 0.9715713262557983, 'coordinate': [np.float32(249.7707), np.float32(654.4779), np.float32(1169.3091), np.float32(1524.1571)]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9423873424530029, 'coordinate': [np.float32(182.03171), np.float32(1545.5562), np.float32(1231.0365), np.float32(1687.3373)]}, {'cls_id': 8, 'label': 'table', 'score': 0.9247739911079407, 'coordinate': [np.float32(209.77914), np.float32(2786.1228), np.float32(1195.0577), np.float32(3004.3755)]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8986274600028992, 'coordinate': [np.float32(1323.9246), np.float32(1534.1371), np.float32(1640.1566), np.float32(1577.3286)]}, {'cls_id': 9, 'label': 'table_title', 'score': 0.8700641989707947, 'coordinate': [np.float32(175.86523), np.float32(2533.2139), np.float32(1230.681), np.float32(2773.9707)]}, {'cls_id': 12, 'label': 'footnote', 'score': 0.8352326154708862, 'coordinate': [np.float32(192.85417), np.float32(3055.635), np.float32(878.5344), np.float32(3100.5542)]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': array([[[ 184,  223],\n",
      "        ...,\n",
      "        [ 183,  271]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 204, 3066],\n",
      "        ...,\n",
      "        [ 205, 3101]]], shape=(104, 4, 2), dtype=int16), 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.4, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1], shape=(104,)), 'text_rec_score_thresh': 0, 'return_word_box': False, 'rec_texts': ['which contains 260 images collected from [2]. Examples', 'Tab. 4 summarizes the results. The CRNN outper', 'are shown in Fig. 5.a; 2) “Synthesized\", which is created', 'forms the two commercial systems by a large margin. The', 'from“Clean”, using the augmentation strategy mentioned', 'Capella Scan and PhotoScore systems perform reasonably', ' above. It contains 20O samples, some of which are shown', 'well on the Clean dataset, but their performances drop sig-', 'in Fig. 5.b: 3) “Real-World\", which contains 200 images', 'nificantly on synthesized and real-world data. The main', 'of score fragments taken from music books with a phone', ' reason is that they rely on robust binarization to detect staff', 'camera. Examples are shown in Fig. 5.c.', 'lines and notes, but the binarization step often fails on syn-', 'thesized and real-world data due to bad lighting condition,', 'noise corruption and cluttered background. The CRNN, on', '(a)', 'the other hand, uses convolutional features that are highly', ' robust to noises and distortions. Besides,recurrent layers in', 'CRNN can utilize contextual information in the score. Each', 'note is recognized not only itself, but also by the nearby', 'notes. Consequently, some notes can be recognized by com-', '（b)', 'paring them with the nearby notes,e.g. contrasting their', 'vertical positions.', 'The results have shown the generality of CRNN, in that', ' it can be readily applied to other image-based sequence', 'recognition problems, requiring minimal domain knowl-', '', 'edge. Compared with Capella Scan and PhotoScore,our', 'Em', 'CRNN-based system is still preliminary and misses many', '(c)', 'functionalities. But it provides a new scheme for OMR. and', '', 'has shown promising capabilities in pitch recognition', '', '中门', '4. Conclusion', 'Figure 5. (a) Clean musical scores images collected from [2] (b)', 'Synthesized musical score images. (c) Real-world score images', 'In this paper, we have presented a novel neural net-', 'taken with a mobile phone camera.', 'work architecture, called Convolutional Recurrent Neural', 'Since we have limited training data, we use a simpli-', 'Network(CRNN), which integrates the advantages of both', 'fied CRNN configuration in order to reduce model capac-', 'Convolutional Neural Networks (CNN) and Recurrent Neu-', ' ral Networks (RNN). CRNN is able to take input images of', 'ity. Different from the configuration specified in Tab. 1,', 'the 4th and 6th convolution layers are removed, and the', ' varying dimensions and produces predictions with different', '2-layer bidirectional LSTM is replaced by a 2-layer sin-', 'lengths. It directly runs on coarse level labels (e.g. words),', 'requiring no detailed annotations for each individual ele-', 'gle directional LSTM. The network is trained on the pairs', 'of images and corresponding label sequences. Two mea-', 'ment (e.g. characters) in the training phase. Moreover', ' as CRNN abandons fully connected layers used in conven-', 'sures are used for evaluating the recognition performance:', ' tional neural networks, it results in a much more compact', '1) fragment accuracy, i.e. the percentage of score fragments', 'and efficient model. All these properties make CRNN an', 'correctly recognized; 2) average edit distance, i.e. the av-', 'erage edit distance between predicted pitch sequences and', 'excellent approach for image-based sequence recognition.', 'the ground truths. For comparison， we evaluate two com-', 'The experiments on the scene text recognition bench', 'mercial OMR engines, namely the Capella Scan [3] and the', 'marks demonstrate that CRNN achieves superior or highly', 'PhotoScore [4].', 'competitive performance， compared with conventional', 'methods as well as other CNN and RNN based algorithms.', 'Table 4. Comparison of pitch recognition accuracies， among', 'This confirms the advantages of the proposed algorithm. In', 'CRNN and two commercial OMR systems, on the three datasets', 'addition, CRNN significantly outperforms other competi-', 'we have collected. Performances are evaluated by fragment accu-', ' tors on a benchmark for Optical Music Recognition (OMR),', ' racies and average edit distance (\"fragment accuracy/average edit', 'which verifies the generality of CRNN.', 'distance\")', 'Actually, CRNN is a general framework, thus it can be', 'Clean', 'Synthesized', 'Real-World', 'applied to other domains and problems (such as Chinese', 'Capella Scan [3]', '51.9%/1.75', '20.0%/2.31', '43.5%/3.05', 'character recognition)， which involve sequence prediction', 'PhotoScore [4]', '55.0%/2.34', '28.0%/1.85', '20.4%/3.00', 'in images. To further speed up CRNN and make it more', '74.6%/0.37', '81.5 %/0.30', '84.0%/0.30', 'CRNN', 'practical in real-world applications is another direction that', 'is worthy of exploration in the future', '1 We will release the dataset for academic use.'], 'rec_scores': array([0.97257799, ..., 0.98389643], shape=(104,)), 'rec_polys': array([[[ 184,  223],\n",
      "        ...,\n",
      "        [ 183,  271]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 204, 3066],\n",
      "        ...,\n",
      "        [ 205, 3101]]], shape=(104, 4, 2), dtype=int16), 'rec_boxes': array([[ 183, ...,  289],\n",
      "       ...,\n",
      "       [ 204, ..., 3101]], shape=(104, 4), dtype=int16)}, 'table_res_list': [{'cell_box_list': [array([ 227.17104721, ..., 2835.73743439], shape=(4,)), array([ 513.95089722, ..., 2835.51435089], shape=(4,)), array([ 732.52792358, ..., 2835.12431335], shape=(4,)), array([ 964.67727661, ..., 2834.64134979], shape=(4,)), array([ 228.3578968 , ..., 2891.36354828], shape=(4,)), array([ 513.04870605, ..., 2890.92702484], shape=(4,)), array([ 731.69949341, ..., 2890.88444519], shape=(4,)), array([ 964.50411987, ..., 2889.85063934], shape=(4,)), array([ 228.45908356, ..., 2944.20446777], shape=(4,)), array([ 512.20394897, ..., 2943.37437439], shape=(4,)), array([ 732.24185181, ..., 2943.12335205], shape=(4,)), array([ 964.0428772, ..., 2942.4344635], shape=(4,)), array([ 227.96134567, ..., 2996.95195007], shape=(4,)), array([ 512.08084106, ..., 2996.63368225], shape=(4,)), array([ 732.16940308, ..., 2995.98326111], shape=(4,)), array([ 963.48007202, ..., 2995.3939209 ], shape=(4,))], 'pred_html': '<html><body><table><tr><td></td><td>Clean</td><td>Synthesized</td><td>Real-World</td></tr><tr><td>Capella Scan [3]</td><td>51.9%/1.75</td><td>20.0%/2.31</td><td>43.5%/3.05</td></tr><tr><td>PhotoScore [4]</td><td>55.0%/2.34</td><td>28.0%/1.85</td><td>20.4%/3.00</td></tr><tr><td>CRNN</td><td>74.6%/0.37</td><td>81.5 %/0.30</td><td>84.0%/0.30</td></tr></table></body></html>', 'table_ocr_pred': {'rec_polys': array([[[ 582, 2791],\n",
      "        ...,\n",
      "        [ 582, 2829]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 224, 2961],\n",
      "        ...,\n",
      "        [ 228, 3009]]], shape=(15, 4, 2), dtype=int16), 'rec_texts': ['Clean', 'Synthesized', 'Real-World', 'Capella Scan [3]', '51.9%/1.75', '20.0%/2.31', '43.5%/3.05', 'PhotoScore [4]', '55.0%/2.34', '28.0%/1.85', '20.4%/3.00', '74.6%/0.37', '81.5 %/0.30', '84.0%/0.30', 'CRNN'], 'rec_scores': array([0.99941885, ..., 0.99941224], shape=(15,)), 'rec_boxes': [[582, 2791, 676, 2829], [762, 2791, 939, 2829], [991, 2784, 1168, 2826], [242, 2846, 489, 2888], [540, 2839, 707, 2885], [769, 2846, 932, 2881], [998, 2843, 1161, 2877], [235, 2898, 464, 2939], [536, 2894, 707, 2936], [765, 2890, 936, 2936], [994, 2894, 1161, 2932], [537, 2953, 710, 2991], [762, 2949, 936, 2991], [987, 2942, 1165, 2988], [224, 2952, 344, 3009]]}}]}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = pipeline.predict(\"/media/tom/Code/pcb_defect/ProdVision_Server/media/data_test/1507.05717v1_page-0008.jpg\")\n",
    "for res in output:\n",
    "    res.print() ## Print the predicted structured output\n",
    "    res.save_to_img(\"./output/\")\n",
    "    res.save_to_xlsx(\"./output/\")\n",
    "    res.save_to_html(\"./output/\")\n",
    "    res.save_to_json(\"./output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1333e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomkey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
