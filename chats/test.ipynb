{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import LayoutDetection, TableCellsDetection, TextDetection, TextRecognition\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layout_model():\n",
    "    return LayoutDetection(model_name=\"PP-DocLayoutV2\")\n",
    "\n",
    "def get_table_model():\n",
    "    return TableCellsDetection(model_name=\"RT-DETR-L_wired_table_cell_det\")\n",
    "\n",
    "def get_text_det_model():\n",
    "    return TextDetection(device=\"gpu\",model_name=\"PP-OCRv5_server_det\")\n",
    "\n",
    "def get_text_rec_model():\n",
    "    return TextRecognition(device=\"gpu\", model_name=\"PP-OCRv5_mobile_rec\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaca722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "\n",
    "print(\"Paddle version:\", paddle.__version__)\n",
    "print(\"Is compiled with CUDA:\", paddle.is_compiled_with_cuda())\n",
    "print(\"CUDA device count:\", paddle.device.cuda.device_count())\n",
    "print(\"Current device:\", paddle.get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r\"/media/tom/Code/pcb_defect/ProdVision_Server/media/data_test/1507.05717v1_page-0005.jpg\"\n",
    "layout_model = get_layout_model()\n",
    "table_model = get_table_model()\n",
    "text_det_model = get_text_det_model()\n",
    "text_rec_model = get_text_rec_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec5124",
   "metadata": {},
   "source": [
    "# analysis layout\n",
    "- extract object paragrap\n",
    "- merge paragrap by ( y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49290fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_patches(layout, img_path):\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    data = layout[0][\"boxes\"]\n",
    "\n",
    "    # Tạo tất cả patch\n",
    "    patches = [{\n",
    "        \"patch_id\": i,\n",
    "        \"patch_label\": bbox[\"label\"],\n",
    "        \"patch_positition\": [int(c) for c in bbox[\"coordinate\"]],\n",
    "        \"patch_score\": bbox[\"score\"],\n",
    "        \"patch\": img[int(bbox[\"coordinate\"][1]):int(bbox[\"coordinate\"][3]),\n",
    "                     int(bbox[\"coordinate\"][0]):int(bbox[\"coordinate\"][2])]\n",
    "    } for i, bbox in enumerate(data)]\n",
    "\n",
    "    # Hàm merge các patch text liên tiếp\n",
    "    def merge_text_patches(patch_group):\n",
    "        xs = [x for p in patch_group for x in (p[\"patch_positition\"][0], p[\"patch_positition\"][2])]\n",
    "        ys = [y for p in patch_group for y in (p[\"patch_positition\"][1], p[\"patch_positition\"][3])]\n",
    "        scores = [p[\"patch_score\"] for p in patch_group]\n",
    "\n",
    "        new_position = [min(xs), min(ys), max(xs), max(ys)]\n",
    "        x1, y1, x2, y2 = map(int, new_position)\n",
    "        merged_patch = img[y1:y2, x1:x2]\n",
    "\n",
    "        return {\n",
    "            \"patch_id\": patch_group[0][\"patch_id\"],\n",
    "            \"patch_label\": \"text\",\n",
    "            \"patch_positition\": new_position,\n",
    "            \"patch_score\": max(scores),\n",
    "            \"patch\": merged_patch\n",
    "        }\n",
    "\n",
    "    # Merge patch text liên tiếp\n",
    "    finally_patches = []\n",
    "    temp_patches = []\n",
    "\n",
    "    for patch in patches:\n",
    "        if patch[\"patch_label\"] == \"text\":\n",
    "            temp_patches.append(patch)\n",
    "        else:\n",
    "            if temp_patches:\n",
    "                finally_patches.append(merge_text_patches(temp_patches))\n",
    "                temp_patches = []\n",
    "            finally_patches.append(patch)\n",
    "\n",
    "    if temp_patches:\n",
    "        finally_patches.append(merge_text_patches(temp_patches))\n",
    "\n",
    "    return finally_patches\n",
    "# patch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a01c24",
   "metadata": {},
   "source": [
    "# sort list poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5853c",
   "metadata": {},
   "source": [
    "# warped polys to patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def warp_polys_to_patches(text_det):\n",
    "    \"\"\"\n",
    "    Từ kết quả text detection, sort polygon theo trục y, warp từng polygon thành patch chữ nhật.\n",
    "    \n",
    "    Args:\n",
    "        text_det: output của text_det_model.predict, dạng list/dict như text_det[0]\n",
    "    \n",
    "    Returns:\n",
    "        warped_patches: list các dict, mỗi dict gồm:\n",
    "            - 'patch': patch ảnh đã warp\n",
    "            - 'bboxes': bbox trong patch\n",
    "            - 'poly': polygon sau warp (int)\n",
    "    \"\"\"\n",
    "    list_poly = text_det[0][\"dt_polys\"]\n",
    "    img_input = text_det[0][\"input_img\"]\n",
    "\n",
    "    centers_y = np.array([np.mean(np.array(poly)[:,1]) for poly in list_poly])\n",
    "    sorted_idx = np.argsort(centers_y)\n",
    "    list_poly_sorted = [list_poly[i] for i in sorted_idx]\n",
    "\n",
    "    warped_patches = []\n",
    "\n",
    "    for poly in list_poly_sorted:\n",
    "        poly = np.array(poly, dtype=np.float32)\n",
    "\n",
    "        if len(poly) > 4:\n",
    "            hull = cv2.convexHull(poly).squeeze()\n",
    "            src_pts = hull[:4] if hull.shape[0] >= 4 else hull\n",
    "        else:\n",
    "            src_pts = poly\n",
    "\n",
    "        while src_pts.shape[0] < 4:\n",
    "            src_pts = np.vstack([src_pts, src_pts[-1]])\n",
    "\n",
    "        min_x, min_y = src_pts.min(axis=0)\n",
    "        max_x, max_y = src_pts.max(axis=0)\n",
    "        width = int(max_x - min_x)\n",
    "        height = int(max_y - min_y)\n",
    "        dst_pts = np.array([[0,0],[width-1,0],[width-1,height-1],[0,height-1]], dtype=np.float32)\n",
    "\n",
    "        M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "        warped = cv2.warpPerspective(img_input, M, (width, height))\n",
    "\n",
    "        poly_warped = cv2.perspectiveTransform(src_pts.reshape(-1,1,2), M).reshape(-1,2)\n",
    "        min_x_w, min_y_w = poly_warped.min(axis=0)\n",
    "        max_x_w, max_y_w = poly_warped.max(axis=0)\n",
    "        bbox = [int(min_x_w), int(min_y_w), int(max_x_w), int(max_y_w)]\n",
    "\n",
    "        warped_patches.append({\n",
    "            \"patch\": warped,\n",
    "            \"bboxes\": bbox,\n",
    "            \"poly\": poly_warped.astype(int)\n",
    "        })\n",
    "\n",
    "    return warped_patches\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd76569",
   "metadata": {},
   "source": [
    "# Text recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31adecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recognize_text_from_patches(warped_patches, text_rec_model, batch_size=1):\n",
    "    \"\"\"\n",
    "    Duyệt qua danh sách patch, predict text và trả về list dict gồm text, bbox, score, font.\n",
    "\n",
    "    Args:\n",
    "        warped_patches: list các dict, mỗi dict gồm 'patch', 'bboxes', 'poly'\n",
    "        text_rec_model: model nhận dạng text\n",
    "        batch_size: batch size khi predict\n",
    "\n",
    "    Returns:\n",
    "        text_patch: list dict, mỗi dict gồm 'text', 'bbox', 'score', 'front'\n",
    "    \"\"\"\n",
    "    text_patch = []\n",
    "\n",
    "    for patch_dict in warped_patches:\n",
    "        img = patch_dict['patch']\n",
    "        text_lines = text_rec_model.predict(img, batch_size=batch_size)\n",
    "\n",
    "        result = {\n",
    "            \"text\": text_lines[0][\"rec_text\"],\n",
    "            \"bbox\": patch_dict['bboxes'],\n",
    "            \"score\": text_lines[0][\"rec_score\"],\n",
    "            \"front\": text_lines[0][\"vis_font\"]\n",
    "        }\n",
    "\n",
    "        text_patch.append(result)\n",
    "\n",
    "    return text_patch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b10e6b",
   "metadata": {},
   "source": [
    "# Table processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_to_bbox(poly):\n",
    "    \"\"\"\n",
    "    Chuyển polygon (N,2) thành bbox (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    poly = np.array(poly)\n",
    "    x_min = np.min(poly[:, 0])\n",
    "    y_min = np.min(poly[:, 1])\n",
    "    x_max = np.max(poly[:, 0])\n",
    "    y_max = np.max(poly[:, 1])\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def extract_table_text_from_patch(table_patch, table_model, text_det_model, text_rec_model):\n",
    "    \"\"\"\n",
    "    Hàm trích xuất text từ các ô trong bảng dựa trên patch ảnh.\n",
    "\n",
    "    Args:\n",
    "        table_patch (np.array): patch ảnh chứa bảng.\n",
    "        table_model: model dự đoán bảng.\n",
    "        text_det_model: model phát hiện text trong cell.\n",
    "        text_rec_model: model nhận diện text.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: danh sách kết quả với cell_bbox, text, score, font.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    cells = table_model.predict(table_patch)\n",
    "\n",
    "    for cell in cells[0][\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = cell[\"coordinate\"]\n",
    "        bbox_cell = [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "        patch = table_patch[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        line_text_det = text_det_model.predict(patch)\n",
    "        polys = line_text_det[0][\"dt_polys\"]\n",
    "        line_img = line_text_det[0][\"input_img\"]\n",
    "\n",
    "        for poly in polys:\n",
    "            bbox = poly_to_bbox(poly=poly)\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "            line_patch = line_img[y_min:y_max, x_min:x_max]\n",
    "            text_info = text_rec_model.predict(line_patch)[0]\n",
    "\n",
    "            result.append({\n",
    "                \"cell_bbox\": bbox_cell,\n",
    "                \"text\": text_info[\"rec_text\"],\n",
    "                \"score\": text_info[\"rec_score\"],\n",
    "                \"front\": text_info[\"vis_font\"]\n",
    "            })\n",
    "\n",
    "    return result\n",
    "import numpy as np\n",
    "def reconstruct_table_from_result(result, y_threshold=5):\n",
    "    \"\"\"\n",
    "    Tái tạo bảng 2D từ kết quả extract_table_text_from_patch,\n",
    "    dùng threshold để gom các cell cùng hàng.\n",
    "\n",
    "    Args:\n",
    "        result (list[dict]): danh sách các ô với bbox và text\n",
    "        y_threshold (int): khoảng cách tối đa về y để coi cùng hàng\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: bảng 2D\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for item in result:\n",
    "        cell_bbox = tuple(item[\"cell_bbox\"])\n",
    "        text = item[\"text\"]\n",
    "        cells.append((cell_bbox, text))\n",
    "    \n",
    "    # sort theo y_min, x_min\n",
    "    cells.sort(key=lambda x: (x[0][1], x[0][0]))\n",
    "\n",
    "    table = []\n",
    "    current_row = []\n",
    "    current_y_min = None\n",
    "\n",
    "    for bbox, text in cells:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        if current_y_min is None:\n",
    "            current_y_min = y_min\n",
    "        # Nếu cell lệch quá nhiều về y -> hàng mới\n",
    "        if y_min - current_y_min > y_threshold:\n",
    "            table.append(current_row)\n",
    "            current_row = []\n",
    "            current_y_min = y_min\n",
    "        current_row.append((x_min, text))  # lưu x_min để sort cột sau\n",
    "\n",
    "    if current_row:\n",
    "        table.append(current_row)\n",
    "\n",
    "    # sort từng row theo x_min và chỉ lấy text\n",
    "    table_2d = []\n",
    "    for row in table:\n",
    "        row_sorted = [text for x, text in sorted(row, key=lambda x: x[0])]\n",
    "        table_2d.append(row_sorted)\n",
    "\n",
    "    return table_2d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table_2d_to_df(table_2d):\n",
    "    \"\"\"\n",
    "    Chuyển bảng 2D thành pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        table_2d (list[list[str]]): bảng 2D\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Tìm số cột lớn nhất\n",
    "    max_cols = max(len(row) for row in table_2d)\n",
    "    \n",
    "    # Bổ sung các cell trống nếu row ngắn hơn\n",
    "    normalized_table = [row + [\"\"]*(max_cols - len(row)) for row in table_2d]\n",
    "    \n",
    "    return pd.DataFrame(normalized_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd15f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list_of_dict(df):\n",
    "    \"\"\"\n",
    "    Chuyển pandas DataFrame thành list of dict.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): bảng dữ liệu\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: mỗi dict là một row\n",
    "    \"\"\"\n",
    "    # Lấy tên cột từ DataFrame\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # convert từng row thành dict\n",
    "    return df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61791b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(\n",
    "    img_path,\n",
    "    layout_model,\n",
    "    text_det_model,\n",
    "    text_rec_model,\n",
    "    table_model\n",
    "):\n",
    "\n",
    "    # 1. Layout detection\n",
    "    layout_result = layout_model.predict(\n",
    "        img_path,\n",
    "        batch_size=1,\n",
    "        layout_nms=True\n",
    "    )\n",
    "\n",
    "    # 2. Merge layout patches\n",
    "    merged_patches = extract_patches(layout_result, img_path)\n",
    "\n",
    "    document_paragraphs = []\n",
    "\n",
    "    for patch_info in merged_patches:\n",
    "        patch_img = patch_info[\"patch\"]\n",
    "        patch_idx = patch_info[\"patch_id\"]\n",
    "        patch_type = patch_info[\"patch_label\"]\n",
    "        patch_coords = patch_info[\"patch_positition\"]\n",
    "        patch_conf = patch_info[\"patch_score\"]\n",
    "\n",
    "        # 3. Process by patch type\n",
    "        if patch_type == \"table\":\n",
    "            table_result = extract_table_text_from_patch(table_patch=patch_img, \n",
    "                                                         table_model=table_model, \n",
    "                                                         text_det_model=text_det_model, \n",
    "                                                         text_rec_model=text_rec_model)\n",
    "            # print(table_result)\n",
    "            table_2d = reconstruct_table_from_result(table_result)\n",
    "            df = table_2d_to_df(table_2d)\n",
    "            recognized_text =  df_to_list_of_dict(df)\n",
    "            # df.to_csv(\"/media/tom/Code/pcb_defect/ProdVision_Server/chats/outputs/tabel.csv\")    \n",
    "\n",
    "        else:\n",
    "            det_result = text_det_model.predict(\n",
    "                patch_img,\n",
    "                batch_size=1\n",
    "            )\n",
    "\n",
    "            rectified_patches = warp_polys_to_patches(\n",
    "                det_result\n",
    "            )\n",
    "\n",
    "            recognized_text = recognize_text_from_patches(\n",
    "                rectified_patches,\n",
    "                text_rec_model\n",
    "            )\n",
    "\n",
    "        # 4. Collect result\n",
    "        paragraph_entry = {\n",
    "            \"patch_id\": patch_idx,\n",
    "            \"patch_label\": patch_type,\n",
    "            \"patch_position\": patch_coords,\n",
    "            \"patch_score\": patch_conf,\n",
    "            \"text_dict\": recognized_text\n",
    "        }\n",
    "\n",
    "        document_paragraphs.append(paragraph_entry)\n",
    "\n",
    "    return document_paragraphs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0fd8607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cell_bbox': [np.float32(6.2295895), np.float32(217.62062), np.float32(328.89517), np.float32(258.95178)], 'text': 'Convolution', 'score': 0.99945467710495, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.253868), np.float32(258.80533), np.float32(328.8996), np.float32(300.03705)], 'text': 'MaxPooling', 'score': 0.9996495246887207, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.068519), np.float32(176.4237), np.float32(328.90393), np.float32(217.78813)], 'text': 'Map-to-Sequence', 'score': 0.9996843338012695, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.3051014), np.float32(340.86072), np.float32(328.90775), np.float32(382.14307)], 'text': 'Convolution', 'score': 0.9994373321533203, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.1158514), np.float32(423.12305), np.float32(328.86877), np.float32(464.31818)], 'text': 'Convolution', 'score': 0.9994513988494873, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.77377), np.float32(299.86685), np.float32(756.1351), np.float32(341.0933)], 'text': '-', 'score': 0.7020989656448364, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.73337), np.float32(176.41177), np.float32(756.2675), np.float32(217.8308)], 'text': '-', 'score': 0.9595240354537964, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.095678), np.float32(464.16943), np.float32(328.86584), np.float32(505.36087)], 'text': 'MaxPooling', 'score': 0.9997531771659851, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.71292), np.float32(258.69476), np.float32(756.11285), np.float32(299.99316)], 'text': 'Window:1 × 2, s:2', 'score': 0.9745858311653137, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.2519855), np.float32(299.8196), np.float32(328.89987), np.float32(341.04092)], 'text': 'BatchNormalization', 'score': 0.9997289180755615, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.3024135), np.float32(710.6954), np.float32(328.79752), np.float32(751.89197)], 'text': 'Convolution', 'score': 0.999441385269165, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.75256), np.float32(381.9922), np.float32(756.09796), np.float32(423.18933)], 'text': '-', 'score': 0.9690600037574768, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.62378), np.float32(93.95494), np.float32(756.1785), np.float32(135.18599)], 'text': '#hidden units:256', 'score': 0.9988038539886475, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.2578144), np.float32(381.96585), np.float32(328.87866), np.float32(423.2465)], 'text': 'BatchNormalization', 'score': 0.9997172951698303, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.3875527), np.float32(669.57263), np.float32(328.8126), np.float32(710.8293)], 'text': 'MaxPooling', 'score': 0.9997540712356567, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.2661943), np.float32(93.97557), np.float32(328.88416), np.float32(135.30084)], 'text': 'Bidirectional-LSTM', 'score': 0.999719500541687, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.1588974), np.float32(135.08661), np.float32(328.87772), np.float32(176.63934)], 'text': 'Bidirectional-LSTM', 'score': 0.9997565746307373, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.68237), np.float32(134.96129), np.float32(756.2195), np.float32(176.56017)], 'text': '#hidden units:256', 'score': 0.9988523125648499, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.2116385), np.float32(505.24854), np.float32(328.91394), np.float32(546.24164)], 'text': 'Convolution', 'score': 0.99945467710495, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.3248777), np.float32(628.4475), np.float32(328.84244), np.float32(669.68896)], 'text': 'Convolution', 'score': 0.9994373321533203, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.7129), np.float32(217.60022), np.float32(756.0021), np.float32(258.86008)], 'text': '#maps:512, k:2 × 2, s:1, p:0', 'score': 0.9914801716804504, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.52954), np.float32(669.4646), np.float32(756.30475), np.float32(710.7282)], 'text': 'Window:2 × 2, s:2', 'score': 0.9694944024085999, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.60083), np.float32(463.93762), np.float32(756.0579), np.float32(505.2435)], 'text': 'Window:1 × 2, s:2', 'score': 0.9846011996269226, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.2013083), np.float32(546.2005), np.float32(328.86755), np.float32(587.36725)], 'text': 'Convolution', 'score': 0.9994456768035889, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.262248), np.float32(587.2387), np.float32(328.85608), np.float32(628.58594)], 'text': 'MaxPooling', 'score': 0.9997513890266418, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.64703), np.float32(340.8401), np.float32(756.0718), np.float32(382.11423)], 'text': '#maps:512, k:3 × 3, s:1, p:1', 'score': 0.9767746925354004, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.0220604), np.float32(3.8707325), np.float32(328.92432), np.float32(48.635147)], 'text': 'Type', 'score': 0.9987867474555969, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.60196), np.float32(710.52246), np.float32(756.4044), np.float32(751.771)], 'text': '#maps:64, k:3 × 3, s:1, p: 1', 'score': 0.9762636423110962, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.44296), np.float32(3.733973), np.float32(755.9949), np.float32(48.51125)], 'text': 'igurations', 'score': 0.9986271858215332, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.44296), np.float32(3.733973), np.float32(755.9949), np.float32(48.51125)], 'text': 'Confi', 'score': 0.995214581489563, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.60547), np.float32(587.2049), np.float32(756.14746), np.float32(628.56116)], 'text': 'Window:2 × 2, s:2', 'score': 0.9936091303825378, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.221062), np.float32(49.03824), np.float32(328.91223), np.float32(94.0522)], 'text': 'Transcription', 'score': 0.9996893405914307, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.70657), np.float32(422.97073), np.float32(755.94745), np.float32(464.117)], 'text': '#maps:512, k:3 × 3, s:1, p: 1', 'score': 0.9589914679527283, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.69336), np.float32(505.01813), np.float32(756.0974), np.float32(546.2957)], 'text': '#maps:256, k:3 × 3, s:1, p: 1', 'score': 0.9830096960067749, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.68854), np.float32(628.35144), np.float32(756.3262), np.float32(669.69495)], 'text': '#maps:128, k:3 × 3, s:1, p: 1', 'score': 0.9813613891601562, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.71167), np.float32(546.10925), np.float32(756.14703), np.float32(587.4197)], 'text': '#maps:256, k:3 × 3, s:1, p: 1', 'score': 0.9834614992141724, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(6.1643405), np.float32(751.7044), np.float32(328.97107), np.float32(789.4087)], 'text': 'Input', 'score': 0.9997051954269409, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}, {'cell_bbox': [np.float32(328.6216), np.float32(751.49084), np.float32(756.3116), np.float32(789.21564)], 'text': 'W × 32 gray-scale image', 'score': 0.9914535284042358, 'front': <paddlex.utils.fonts.Font object at 0x7b62eb911150>}]\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/media/tom/Code/pcb_defect/ProdVision_Server/media/data_test/1507.05717v1_page-0005.jpg\"\n",
    "\n",
    "\n",
    "# Access individual dimensions\n",
    "document_paragraphs = process_document(\n",
    "    img_path=img_path,\n",
    "    layout_model=layout_model,\n",
    "    text_det_model=text_det_model,\n",
    "    text_rec_model=text_rec_model,\n",
    "    table_model=table_model\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "79d4badb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'patch_id': 0,\n",
       "  'patch_label': 'text',\n",
       "  'patch_position': [195, 305, 1203, 1301],\n",
       "  'patch_score': 0.9885912537574768,\n",
       "  'text_dict': [{'text': 'where yi is the sequence produced by the recurrent and con-',\n",
       "    'bbox': [0, 0, 988, 43],\n",
       "    'score': 0.9841119050979614,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'volutional layers from Ii. This objective function calculates',\n",
       "    'bbox': [0, 0, 991, 47],\n",
       "    'score': 0.982840359210968,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'a cost value directly from an image and its ground truth',\n",
       "    'bbox': [0, 0, 991, 43],\n",
       "    'score': 0.9845973253250122,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'label sequence. Therefore, the network can be end-to-end',\n",
       "    'bbox': [0, 0, 991, 45],\n",
       "    'score': 0.9842114448547363,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'trained on pairs of images and sequences, eliminating the',\n",
       "    'bbox': [0, 0, 991, 44],\n",
       "    'score': 0.9859133362770081,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'procedure of manually labeling all individual components',\n",
       "    'bbox': [0, 0, 991, 44],\n",
       "    'score': 0.9865004420280457,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'in training images.',\n",
       "    'bbox': [0, 0, 317, 45],\n",
       "    'score': 0.9988006949424744,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'The network is trained with stochastic gradient descent',\n",
       "    'bbox': [0, 0, 945, 48],\n",
       "    'score': 0.9909049868583679,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': '(SGD). Gradients are calculated by the back-propagation al-',\n",
       "    'bbox': [0, 0, 989, 44],\n",
       "    'score': 0.9940615892410278,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'gorithm. In particular, in the transcription layer, error dif-',\n",
       "    'bbox': [0, 0, 988, 43],\n",
       "    'score': 0.983465313911438,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ferentials are back-propagated with the forward-backward',\n",
       "    'bbox': [0, 0, 993, 46],\n",
       "    'score': 0.9844364523887634,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'algorithm, as described in [15]. In the recurrent layers, the',\n",
       "    'bbox': [0, 0, 992, 46],\n",
       "    'score': 0.9912057518959045,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'Back-Propagation Through Time (BPTT) is applied to cal-',\n",
       "    'bbox': [0, 0, 991, 45],\n",
       "    'score': 0.9902961850166321,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'culate the error differentials.',\n",
       "    'bbox': [0, 0, 474, 38],\n",
       "    'score': 0.9935334324836731,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'For optimization, we use the ADADELTA [37] to au-',\n",
       "    'bbox': [0, 0, 941, 43],\n",
       "    'score': 0.9787776470184326,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'tomatically calculate per-dimension learning rates. Com-',\n",
       "    'bbox': [0, 0, 991, 44],\n",
       "    'score': 0.9840084314346313,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'pared with the conventional momentum [31] method,',\n",
       "    'bbox': [0, 0, 994, 46],\n",
       "    'score': 0.9770667552947998,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ADADELTA requires no manual setting of a learning',\n",
       "    'bbox': [0, 0, 993, 45],\n",
       "    'score': 0.9847778081893921,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'rate. More importantly, we find that optimization using',\n",
       "    'bbox': [0, 0, 991, 43],\n",
       "    'score': 0.9817352294921875,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ADADELTA converges faster than the momentum method.',\n",
       "    'bbox': [0, 0, 988, 41],\n",
       "    'score': 0.9874778389930725,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 3,\n",
       "  'patch_label': 'paragraph_title',\n",
       "  'patch_position': [199, 1340, 538, 1397],\n",
       "  'patch_score': 0.9362549185752869,\n",
       "  'text_dict': [{'text': '3. Experiments',\n",
       "    'bbox': [0, 0, 322, 42],\n",
       "    'score': 0.9947844743728638,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 4,\n",
       "  'patch_label': 'text',\n",
       "  'patch_position': [195, 1425, 1203, 1926],\n",
       "  'patch_score': 0.9887940287590027,\n",
       "  'text_dict': [{'text': 'To evaluate the effectiveness of the proposed CRNN',\n",
       "    'bbox': [0, 0, 939, 41],\n",
       "    'score': 0.9785956740379333,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'model, we conducted experiments on standard benchmarks',\n",
       "    'bbox': [0, 0, 985, 42],\n",
       "    'score': 0.9868367314338684,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'for scene text recognition and musical score recognition,',\n",
       "    'bbox': [0, 0, 988, 41],\n",
       "    'score': 0.9791666865348816,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'which are both challenging vision tasks. The datasets and',\n",
       "    'bbox': [0, 0, 987, 41],\n",
       "    'score': 0.9888548851013184,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'setting for training and testing are given in Sec. 3.1, the de-',\n",
       "    'bbox': [0, 0, 987, 43],\n",
       "    'score': 0.985643744468689,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'tailed settings of CRNN for scene text images is provided',\n",
       "    'bbox': [0, 0, 990, 43],\n",
       "    'score': 0.9897933006286621,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'in Sec. 3.2, and the results with the comprehensive compar-',\n",
       "    'bbox': [0, 0, 990, 50],\n",
       "    'score': 0.9866915941238403,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'isons are reported in Sec. 3.3. To further demonstrate the',\n",
       "    'bbox': [0, 0, 990, 41],\n",
       "    'score': 0.991485595703125,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'generality of CRNN, we verify the proposed algorithm on a',\n",
       "    'bbox': [0, 0, 993, 46],\n",
       "    'score': 0.9898152947425842,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'music score recognition task in Sec. 3.4.',\n",
       "    'bbox': [0, 0, 675, 44],\n",
       "    'score': 0.9703541398048401,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 5,\n",
       "  'patch_label': 'paragraph_title',\n",
       "  'patch_position': [198, 1950, 464, 2001],\n",
       "  'patch_score': 0.936479389667511,\n",
       "  'text_dict': [{'text': '3.1. Datasets',\n",
       "    'bbox': [0, 0, 255, 44],\n",
       "    'score': 0.9928405284881592,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 6,\n",
       "  'patch_label': 'text',\n",
       "  'patch_position': [195, 2028, 1203, 2977],\n",
       "  'patch_score': 0.9889647960662842,\n",
       "  'text_dict': [{'text': 'For all the experiments for scene text recognition, we',\n",
       "    'bbox': [0, 0, 941, 44],\n",
       "    'score': 0.9816276431083679,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'use the synthetic dataset (Synth) released by Jaderberg et',\n",
       "    'bbox': [0, 0, 993, 45],\n",
       "    'score': 0.9841917157173157,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'al. [20] as the training data. The dataset contains 8 millions',\n",
       "    'bbox': [0, 0, 990, 43],\n",
       "    'score': 0.9721846580505371,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'training images and their corresponding ground truth words.',\n",
       "    'bbox': [0, 0, 987, 41],\n",
       "    'score': 0.9866990447044373,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'Such images are generated by a synthetic text engine and',\n",
       "    'bbox': [0, 0, 991, 45],\n",
       "    'score': 0.9882788062095642,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'are highly realistic. Our network is trained on the synthetic',\n",
       "    'bbox': [0, 0, 989, 42],\n",
       "    'score': 0.9902013540267944,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'data once, and tested on all other real-world test datasets',\n",
       "    'bbox': [0, 0, 987, 38],\n",
       "    'score': 0.9777527451515198,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'without any fine-tuning on their training data. Even though',\n",
       "    'bbox': [0, 0, 988, 41],\n",
       "    'score': 0.9971848726272583,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'the CRNN model is purely trained with synthetic text data,',\n",
       "    'bbox': [0, 0, 991, 45],\n",
       "    'score': 0.9900726675987244,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': ' it works well on real images from standard text recognition',\n",
       "    'bbox': [0, 0, 994, 48],\n",
       "    'score': 0.968618631362915,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'benchmarks.',\n",
       "    'bbox': [0, 0, 217, 39],\n",
       "    'score': 0.9996518492698669,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'Four popular benchmarks for scene text recognition are',\n",
       "    'bbox': [0, 0, 943, 44],\n",
       "    'score': 0.9931758642196655,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'used for performance evaluation, namely ICDAR 2003',\n",
       "    'bbox': [0, 0, 992, 45],\n",
       "    'score': 0.9810880422592163,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': '(IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and',\n",
       "    'bbox': [0, 0, 989, 45],\n",
       "    'score': 0.9772663712501526,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'Street View Text (SVT).',\n",
       "    'bbox': [0, 0, 408, 41],\n",
       "    'score': 0.9890088438987732,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'IC03 [27] test dataset contains 251 scene images with la-',\n",
       "    'bbox': [0, 0, 937, 41],\n",
       "    'score': 0.9873849749565125,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'beled text bounding boxes. Following Wang et al. [34], we',\n",
       "    'bbox': [0, 0, 990, 41],\n",
       "    'score': 0.9922217130661011,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ignore images that either contain non-alphanumeric charac-',\n",
       "    'bbox': [0, 0, 989, 41],\n",
       "    'score': 0.9911683797836304,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ters or have less than three characters, and get a test set with',\n",
       "    'bbox': [0, 0, 991, 42],\n",
       "    'score': 0.9782102108001709,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 9,\n",
       "  'patch_label': 'figure_title',\n",
       "  'patch_position': [1273, 290, 2278, 425],\n",
       "  'patch_score': 0.9716778993606567,\n",
       "  'text_dict': [{'text': 'Table 1. Network configuration summary. The first row is the top',\n",
       "    'bbox': [0, 0, 986, 36],\n",
       "    'score': 0.9697068333625793,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': \"layer. 'k', 's'and 'p'stand for kernel size, stride and padding size\",\n",
       "    'bbox': [0, 0, 987, 36],\n",
       "    'score': 0.9754424095153809,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'respectively',\n",
       "    'bbox': [0, 0, 180, 32],\n",
       "    'score': 0.9998180270195007,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 10,\n",
       "  'patch_label': 'table',\n",
       "  'patch_position': [1395, 424, 2156, 1220],\n",
       "  'patch_score': 0.9876899719238281,\n",
       "  'text_dict': [{0: 'Type', 1: 'igurations', 2: 'Confi'},\n",
       "   {0: 'Transcription', 1: '', 2: ''},\n",
       "   {0: 'Bidirectional-LSTM', 1: '#hidden units:256', 2: ''},\n",
       "   {0: 'Bidirectional-LSTM', 1: '#hidden units:256', 2: ''},\n",
       "   {0: 'Map-to-Sequence', 1: '-', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:512, k:2 × 2, s:1, p:0', 2: ''},\n",
       "   {0: 'MaxPooling', 1: 'Window:1 × 2, s:2', 2: ''},\n",
       "   {0: 'BatchNormalization', 1: '-', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:512, k:3 × 3, s:1, p:1', 2: ''},\n",
       "   {0: 'BatchNormalization', 1: '-', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:512, k:3 × 3, s:1, p: 1', 2: ''},\n",
       "   {0: 'MaxPooling', 1: 'Window:1 × 2, s:2', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:256, k:3 × 3, s:1, p: 1', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:256, k:3 × 3, s:1, p: 1', 2: ''},\n",
       "   {0: 'MaxPooling', 1: 'Window:2 × 2, s:2', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:128, k:3 × 3, s:1, p: 1', 2: ''},\n",
       "   {0: 'MaxPooling', 1: 'Window:2 × 2, s:2', 2: ''},\n",
       "   {0: 'Convolution', 1: '#maps:64, k:3 × 3, s:1, p: 1', 2: ''},\n",
       "   {0: 'Input', 1: 'W × 32 gray-scale image', 2: ''}]},\n",
       " {'patch_id': 11,\n",
       "  'patch_label': 'text',\n",
       "  'patch_position': [1272, 1310, 2281, 2015],\n",
       "  'patch_score': 0.9861668944358826,\n",
       "  'text_dict': [{'text': '860 cropped text images. Each test image is associated with',\n",
       "    'bbox': [0, 0, 989, 44],\n",
       "    'score': 0.9873338341712952,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'a 50-words lexicon which is defined by Wang et al. [34]. A',\n",
       "    'bbox': [0, 0, 989, 43],\n",
       "    'score': 0.9903266429901123,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'full lexicon is built by combining all the per-image lexi-',\n",
       "    'bbox': [0, 0, 989, 43],\n",
       "    'score': 0.9725096225738525,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'cons. In addition, we use a 50k words lexicon consisting of',\n",
       "    'bbox': [0, 0, 992, 44],\n",
       "    'score': 0.9948133230209351,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'the words in the Hunspell spell-checking dictionary [1].',\n",
       "    'bbox': [0, 0, 931, 47],\n",
       "    'score': 0.9846919775009155,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'IC13 [24] test dataset inherits most of its data from IC03.',\n",
       "    'bbox': [0, 0, 941, 43],\n",
       "    'score': 0.9892326593399048,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'It contains 1,015 ground truths cropped word images.',\n",
       "    'bbox': [0, 0, 892, 42],\n",
       "    'score': 0.9968389868736267,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'IIT5k [28] contains 3,000 cropped word test images',\n",
       "    'bbox': [0, 0, 942, 48],\n",
       "    'score': 0.967510998249054,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'collected from the Internet. Each image has been associ-',\n",
       "    'bbox': [0, 0, 987, 43],\n",
       "    'score': 0.9863153100013733,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'ated to a 50-words lexicon and a 1k-words lexicon.',\n",
       "    'bbox': [0, 0, 849, 41],\n",
       "    'score': 0.9899134635925293,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'SVT [34] test dataset consists of 249 street view images',\n",
       "    'bbox': [0, 0, 943, 48],\n",
       "    'score': 0.9730411171913147,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'collected from Google Street View. From them 647 word',\n",
       "    'bbox': [0, 0, 993, 46],\n",
       "    'score': 0.9765057563781738,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'images are cropped. Each word image has a 50 words lexi-',\n",
       "    'bbox': [0, 0, 989, 46],\n",
       "    'score': 0.9848345518112183,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'con defined by Wang et al. [34].',\n",
       "    'bbox': [0, 0, 543, 48],\n",
       "    'score': 0.9787458181381226,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 15,\n",
       "  'patch_label': 'paragraph_title',\n",
       "  'patch_position': [1276, 2047, 1833, 2100],\n",
       "  'patch_score': 0.9461008310317993,\n",
       "  'text_dict': [{'text': '3.2. Implementation Details',\n",
       "    'bbox': [0, 0, 544, 43],\n",
       "    'score': 0.9983920454978943,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]},\n",
       " {'patch_id': 16,\n",
       "  'patch_label': 'text',\n",
       "  'patch_position': [1271, 2127, 2283, 2974],\n",
       "  'patch_score': 0.9891149997711182,\n",
       "  'text_dict': [{'text': 'The network confi guration we use in our experiments',\n",
       "    'bbox': [0, 0, 939, 46],\n",
       "    'score': 0.9741703867912292,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'is summarized in Table 1. The architecture of the con-',\n",
       "    'bbox': [0, 0, 990, 42],\n",
       "    'score': 0.9748736023902893,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'volutional layers is based on the VGG-VeryDeep architec-',\n",
       "    'bbox': [0, 0, 988, 44],\n",
       "    'score': 0.9732860326766968,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'tures [32]. A tweak is made in order to make it suitable',\n",
       "    'bbox': [0, 0, 993, 46],\n",
       "    'score': 0.9634546041488647,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'for recognizing English texts. In the 3rd and the 4th max-',\n",
       "    'bbox': [0, 0, 988, 42],\n",
       "    'score': 0.9779350161552429,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'pooling layers, we adopt 1 × 2 sized rectangular pooling',\n",
       "    'bbox': [0, 0, 991, 45],\n",
       "    'score': 0.9780454635620117,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'windows instead of the conventional squared ones. This',\n",
       "    'bbox': [0, 0, 989, 41],\n",
       "    'score': 0.9854339361190796,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'tweak yields feature maps with larger width, hence longer',\n",
       "    'bbox': [0, 0, 989, 41],\n",
       "    'score': 0.9915111660957336,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'feature sequence. For example, an image containing 10',\n",
       "    'bbox': [0, 0, 993, 47],\n",
       "    'score': 0.9708228707313538,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'characters is typically of size 100 × 32, from which a feature',\n",
       "    'bbox': [0, 0, 990, 41],\n",
       "    'score': 0.9808482527732849,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'sequence 25 frames can be generated. This length exceeds',\n",
       "    'bbox': [0, 0, 990, 46],\n",
       "    'score': 0.9854668974876404,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'the lengths of most English words. On top of that, the rect-',\n",
       "    'bbox': [0, 0, 987, 44],\n",
       "    'score': 0.9876003265380859,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'angular pooling windows yield rectangular receptive fields',\n",
       "    'bbox': [0, 0, 987, 46],\n",
       "    'score': 0.9926683306694031,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': '(illustrated in Fig. 2), which are beneficial for recognizing',\n",
       "    'bbox': [0, 0, 987, 45],\n",
       "    'score': 0.9880345463752747,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': \"some characters that have narrow shapes, such as 'i' and '1'.\",\n",
       "    'bbox': [0, 0, 987, 43],\n",
       "    'score': 0.9668351411819458,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'The network not only has deep convolutional layers, but',\n",
       "    'bbox': [0, 0, 942, 43],\n",
       "    'score': 0.9848682284355164,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>},\n",
       "   {'text': 'also has recurrent layers. Both are known to be hard to',\n",
       "    'bbox': [0, 0, 988, 42],\n",
       "    'score': 0.9792530536651611,\n",
       "    'front': <paddlex.utils.fonts.Font at 0x7b62eb911150>}]}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b4489",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f5f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomkey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
